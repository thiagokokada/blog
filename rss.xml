<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
	<channel>
		<title>kokada's blog</title>
		<link>https://github.com/thiagokokada/blog</link>
		<description>dd if=/dev/urandom of=/dev/brain0</description>
		<item>
			<title>Quick bits: nix-shell is cursed</title>
			<guid>https://github.com/thiagokokada/blog/blob/main/2024-07-27/01-quick-bits-nix-shell-is-cursed.md</guid>
			<description>The other day I had to run a PHP project in my machine. I have no idea how PHP ecosystem work, I just wanted to get it to run.
 The easiest way to get a script to run if you use Nix is to use `nix-shell`. As many of you probably know, you can add `nix-shell` as a shebang in your scripts to run them as `./script`. This was a PHP script so I wanted to do the same. Easy right?
 And:
 So it seems that `declare(strict_types=1)` needs to be the first line in a PHP script if used. I removed `declare(strict_types=1)` and while the script works, I don't have enough expertise in PHP to know if this would be safe or not.
 I decided to try something that initially looked really dumb:
 And:
 Wat? I mean, it is not dumb if it works, but this at least looks cursed.
 Eventually I found this comment in a Nix issue talking about cases where `nix-shell` shebang doesn't work. It looks like the classic case of a bug that becomes a feature.</description>
			<link>https://github.com/thiagokokada/blog/blob/main/2024-07-27/01-quick-bits-nix-shell-is-cursed.md</link>
			<pubDate>Sat, 27 Jul 2024 00:00:00 GMT</pubDate>
		</item>
		<item>
			<title>Using GitHub as a (bad) blog platform</title>
			<guid>https://github.com/thiagokokada/blog/blob/main/2024-07-26/02-using-github-as-a-bad-blog-platform.md</guid>
			<description>I finally started a new blog, thanks to the offer of @ratsclub to give me free access to capivaras.dev. But considering how small this blog platform is supposed to be, I want to have at least somewhere to have a backup of my posts. I know Mataroa, the blog platform that capivaras.dev runs, has automatic e-mail backups, but I want something more reliable.
 I am writing all my posts in Markdown (the format that Mataroa supports) files inside neovim anyway, so why not store all my Markdown files in Git? So this is what I did, I now have an unofficial mirror in GitHub.
 While I am here, why not to overcomplicate? Can I make an usable blog platform from GitHub? And by that I don't mean GitHub pages, the repository itself. I mean, it already renders Markdown files by default, so no need to do anything in that space. To reach feature parity with capivaras.dev, I only need to have an index and RSS (since comments are not supported anyway). No need for newsletter since GitHub has a watch feature already.
 After a couple of hours hacking a Python script, you can see the result of this monstrosity here. The script, called `gen_blog.py`, is available at the same repository (here is a permalink). It automatically generates an index at `README.md` with each blog post and a `rss.xml` file at the root of the repository.
 Instead of trying to explain the code, I am going to explain the general idea, because I think that if you want to replicate this idea it is better to rewrite it in a way that you understand. It shouldn't take more than 2 hours in any decent programming language. But if you really want, the script itself is licensed in WTFPL license. The code only uses Python 3's standard library and should work in any relatively recent version (anything newer than 3.9 should work).
 So the idea is basically to organise the repository and the Markdown files in a easy way that makes it trivial to parse in a deterministic way. For example, my repository is organised in the following way:
 Each day that you write a new blog post will be on its own directory. This is nice because Markdown files may include extra files than the posts themselves, e.g.: images, and this organisation make it trivial to organise everything.
 Each post has its own Markdown file. I put a two digit number before each post, to ensure that when publishing multiple posts at the same day I keep them in the same order of publishing. But if you don't care about it, you can just name the files whatever you want.
 Also, I am assuming that each Markdown file has a header starting with `# `, and that is the title of the blog post.
 Using the above organisation, I have this function that scrap the repository and collect the necessary information to generate the index and RSS files:
 Some interesting tidbits: if a Markdown file has a `.` at the start I assume it is a draft post, and ignore it from my scrapper. I added a bunch of `WARN` prints to make sure that the me in the future doesn't do anything dumb. Also, sorting in reverse since reverse chronological order is the one most people expect in blogs (i.e.: more recent blog posts at top).
 After running the function above, I have a resulting dictionary that I can use to generate either a `README.md` file or Markdown:
 To publish a new Post, a basically write a Markdown file, run `./gen_readme.py &gt; README.md` at the root of the repository, and see the magic happens.
 It works much better than I initially antecipated. The `README.md` is properly populated with the titles and links. The RSS is kind empty since it has no description, but it seems to work fine (at least in Inoreader, my RSS reader of choice). I can probably fill the post description with more information if I really want, but it is enough for now. Not sure who is that interested in my writing that will want to use this RSS feed instead the one available in capivaras.dev anyway.
 Also, while I am using GitHub here, the same idea would work in GitLab, Gitea, sr.ht or whatever. As long as your source hub supports Markdown files it should work.
 So that is it. I am not saying this is a good idea for your primary blog platform or whatever, and I still prefer to publish to a platform that doesn't track users or have tons of JavaScript or whatever. But if you want a backup of your posts and you are already writing Markdown anyway, well, there are worse ways to do it I think.</description>
			<link>https://github.com/thiagokokada/blog/blob/main/2024-07-26/02-using-github-as-a-bad-blog-platform.md</link>
			<pubDate>Fri, 26 Jul 2024 00:00:00 GMT</pubDate>
		</item>
		<item>
			<title>Writing NixOS tests for fun and profit</title>
			<guid>https://github.com/thiagokokada/blog/blob/main/2024-07-26/01-writing-nixos-tests-for-fun-and-profit.md</guid>
			<description>I recently started a new side project writing an IPC library in Go for Hyprland, a Window Manager for Wayland.
 Once I got past the Work-in-Progress phase, I realise I had an issue: I wrote some tests, but I was running then inside my system running Hyprland. And the tests themselves were annoying: since they send commands to the current running Hyprland instance, I was having programs being opened and settings being changed, because this was the only way to have a reasonable good confidence that what I was doing was correct. So I need to do like any good developer and implement a CI, but how?
 One approach would be to create something like a mock client and test against my mock. Since this mock wouldn't need a running Hyprland instance the tests could run everywhere (even in non-Linux systems!), but they wouldn't be much useful. Mocks are great for testing business logic, but not really for making sure everything is working correctly.
 I need something more akin to an integration test, but this is tricky. It is not like I am doing integration with e.g.: PostgreSQL that has thousands of libraries available to make integration tests easier, I am doing integration with a Window Manager that is a moving target with multiple breaking changes in each release. And this is where NixOS tests enter, a way to run tests inside Virtual Machines configured in Nix.
 I am a long time NixOS user and commiter, but I never wrote a NixOS test outside of nixpkgs itself. However I knew it was possible, and after doing a quick reading of the Wiki entry about it, I was ready to start.
 The first part is to call `pkgs.testers.runNixOSTest` and configure the machine as any other NixOS system, e.g.:
 A few details that I want to bring to attention. The first one is how easy it is to setup things like a normal user account, add some extra packages we need for testing, add Hyprland itself and configure auto-login. I have no idea how painful it would be to automatise all those steps in e.g.: Ansible, but here we are in a few lines of Nix code. This is, of course, thanks to all the contributors to nixpkgs that implement something that help their own use case, but once combined make it greater than the sum of the parts.
 Second is something that I took a while to figure out: how to enable GPU acceleration inside the VM. You see, Hyprland, different from other Window Managers, requires OpenGL support. This is basically why the flag `-device virtio-gpu-pci` is in `virtualisation.qemu.options`, this enables OpenGL rendering via LLVMPipe, that while being slow since it is rendered in CPU, is sufficient for this case.
 Putting the above code inside a `flake.nix` for reproducibility, I had something similar to:
 I can now run `nix build .#checks.x86_64-linux.testVm -L` to build and run the VM. However it is not really useful right now, since we didn't add any useful code in `testScript`, the core of the NixOS test framework. We can also run `nix build .#checks.x86_64-linux.testVm.driverInteractive` and `./result/bin/nixos-test-driver`: this will start a Python console where we can manually play with the VM (try typing `start_all()` for example).
 The `testScript` is a sequence of Python statements that perform various actions, such as starting VMs, executing commands in the VMs, and so on. More about it in the official documentation. For our case we can start with something like this:
 The first statement, `start_all()`, starts all VMs, in this case we have only one, called `machine`. We send two further commands to `machine`: `wait_for_unit("multi-user.target")` and `wait_for_file("/home/alice/test-finished")`.
 The first command waits until systemd's `multi-user.target` is ready, a good way to ensure that the system is ready for further commands. The second one we wait for a file called `test-finished` to appear in Alice's `$HOME` (basically, a canary), but how can we generate this file?
 Remember that we added `programs.bash.loginShellInit = "Hyprland"`, that automatically starts Hyprland when Alice logs in. We need to modify that command to run the Go tests from our library. The good thing is that Hyprland configuration file supports a `exec-once` command that runs a command during Hyprland launch. We can abuse this to launch a terminal emulator and run our tests:
 So we are basically creating a custom Hyprland config that starts a Kitty terminal emulator, that then launches a shell script that runs the test. Since we have no way to get the results of the test, we pipe the output to a file that we can collect later (e.g.: `machine.succeded("cat /home/alice/test.log")`). And once the script exit, we create the canary file `$HOME/test-finished`, that allows the `testScript` knows that the test finished and it can destroy the VM safely.
 If you want to take a look at the final result, it is here. This tests run in any Linux machine that supports KVM, and also works in GitHub Actions thanks to the the nix-installer-action.
 And now I have a proper CI pipeline in a way that I never imagined would be possible, especially considering how simple it was.</description>
			<link>https://github.com/thiagokokada/blog/blob/main/2024-07-26/01-writing-nixos-tests-for-fun-and-profit.md</link>
			<pubDate>Fri, 26 Jul 2024 00:00:00 GMT</pubDate>
		</item>
	</channel>
</rss>